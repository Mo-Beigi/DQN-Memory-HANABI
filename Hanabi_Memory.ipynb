{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0G2vf4LoFQA",
        "outputId": "fd5a7cfb-648e-479d-cc4c-a377757c0de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hanabi_learning_environment in /usr/local/lib/python3.8/dist-packages (0.0.4)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.8/dist-packages (from hanabi_learning_environment) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi->hanabi_learning_environment) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install hanabi_learning_environment \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2jI3W1E4G8U"
      },
      "source": [
        "Experiment with hanabipy environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "bjkXtZJkcEC9"
      },
      "outputs": [],
      "source": [
        "from hanabi_learning_environment import pyhanabi\n",
        "def card_to_matrix(card):\n",
        "  assert len(card) == 2\n",
        "  color = color_to_int(card[0])\n",
        "\n",
        "  number = -1\n",
        "  try:\n",
        "    number = int(card[1]) - 1\n",
        "  except:\n",
        "    number = -1\n",
        "  mat = np.zeros((5, 5))\n",
        "  try:\n",
        "    mat[color, number] = 1\n",
        "  except:\n",
        "    return mat\n",
        "  return mat\n",
        "\n",
        "def color_to_int(color):\n",
        "  val = -1\n",
        "  if color == 'R':\n",
        "    val = 0\n",
        "  elif color == 'Y':\n",
        "    val = 1\n",
        "  elif color == 'G':\n",
        "    val = 2\n",
        "  elif color == 'W':\n",
        "    val = 3\n",
        "  elif color == 'B':\n",
        "    val = 4\n",
        "  return val\n",
        "\n",
        "def int_to_color(num):\n",
        "  if num == 0:\n",
        "    return 'R'\n",
        "  elif num == 1:\n",
        "    return 'Y'\n",
        "  elif num == 2:\n",
        "    return 'G'\n",
        "  elif num == 3:\n",
        "    return 'W'\n",
        "  elif num == 4:\n",
        "    return 'B'\n",
        "\n",
        "def update_hand_hints(hand_hints, hint):\n",
        "  # print(hint[19])\n",
        "  is_rank = 1 if hint[19] == 'r' else 0\n",
        "  zeros = np.zeros((5, ))\n",
        "  if is_rank:\n",
        "    player_offset = int(hint[17])\n",
        "    player_moving = int(hint[37])\n",
        "    rank = int(hint[24]) - 1\n",
        "    revealed_cards = []\n",
        "    for i in range(46, len(hint), 2):\n",
        "      revealed_cards.append(int(hint[i]))\n",
        "    unrevealed_cards = [i for i in range(5) if i not in revealed_cards]\n",
        "    not_rank = list(range(5))\n",
        "    not_rank.remove(rank)\n",
        "    for card in revealed_cards:\n",
        "      hand_hints[(player_offset + player_moving) % 3, card, :, not_rank] = zeros\n",
        "    for card in unrevealed_cards:\n",
        "      hand_hints[(player_offset + player_moving) % 3, card, :, rank] = zeros\n",
        "  else:\n",
        "    player_offset = int(hint[17])\n",
        "    player_moving = int(hint[38])\n",
        "    color = color_to_int(hint[25])\n",
        "    revealed_cards = []\n",
        "    for i in range(47, len(hint) - 1, 2):\n",
        "      revealed_cards.append(int(hint[i]))\n",
        "    unrevealed_cards = [i for i in range(5) if i not in revealed_cards]\n",
        "    not_color = list(range(5))\n",
        "    not_color.remove(color)\n",
        "    for card in revealed_cards:\n",
        "      hand_hints[(player_offset + player_moving) % 3, card, not_color, :] = zeros\n",
        "    for card in unrevealed_cards:\n",
        "      hand_hints[(player_offset + player_moving) % 3, card, color, :] = zeros\n",
        "\n",
        "  return hand_hints\n",
        "\n",
        "def encode_observation(observation, memory=True):\n",
        "  # If time, add an option to add all observation features into state space (less reliant on memory)\n",
        "  life_tokens = np.zeros((4, ))\n",
        "  life_tokens[observation.life_tokens()] = 1\n",
        "  info_tokens = np.zeros((9, ))\n",
        "  info_tokens[observation.information_tokens()] = 1\n",
        "\n",
        "  fireworks = np.zeros((5, 5))\n",
        "  for i, firework in enumerate(observation.fireworks()):\n",
        "    if firework == 5:\n",
        "      continue\n",
        "    fireworks[i, firework] = 1\n",
        "\n",
        "  hand_hints = np.ones((3, 5, 5, 5))\n",
        "  move_count = 0\n",
        "  for i, move_obj in enumerate(observation.last_moves()):\n",
        "    if move_count >= 3:\n",
        "      break\n",
        "    move = str(move_obj)\n",
        "    # print(move, len(move))\n",
        "    if len(move) > 15:\n",
        "      move_count += 1\n",
        "      if move[2] == 'R':\n",
        "        hand_hints = update_hand_hints(hand_hints, move)\n",
        "\n",
        "  hand_information = np.zeros((3, 5, 5, 5, 3))\n",
        "  for hand_idx, hand in enumerate(observation.observed_hands()):\n",
        "    for card_idx, card in enumerate(hand):\n",
        "      hand_information[hand_idx, card_idx, :, :, 0] = card_to_matrix(str(card))\n",
        "      hand_information[hand_idx, card_idx, :, :, 1] = fireworks\n",
        "  \n",
        "  hand_information[:, :, :, :, 2] = hand_hints\n",
        "\n",
        "  state = {\"life\": life_tokens.reshape(1, 4), \"info\": info_tokens.reshape(1, 9), \"hand\": hand_information.reshape(1, 3, 5, 5, 5, 3)}\n",
        "  return state\n",
        "\n",
        "def encode_possible_moves(possible_moves):\n",
        "  encoding = np.zeros((30,))\n",
        "\n",
        "  for i in range(len(possible_moves)):\n",
        "    \n",
        "    move = str(possible_moves[i])\n",
        "    if move[1] == 'D':\n",
        "      encoding[int(move[9])] = 1\n",
        "    elif move[1] == 'P':\n",
        "      encoding[5 + int(move[6])] = 1\n",
        "    elif move[1] == 'R':\n",
        "      # print(move[18])\n",
        "      player = int(move[16]) - 1\n",
        "      color = 1 if move[18] == 'c' else 0\n",
        "      # print(color)\n",
        "      val = int(move[23]) - 1 if color == 0 else color_to_int(move[24])\n",
        "      idx = 10 + 10 * player + 5 * color + val\n",
        "      # print(idx, move)\n",
        "      encoding[idx] = 1\n",
        "  # print(encoding)\n",
        "  return encoding\n",
        "\n",
        "def possible_move_from_index(possible_moves, index):\n",
        "  is_discard = False\n",
        "  is_play = False\n",
        "  num = 0\n",
        "  if index < 5:\n",
        "    is_discard = True\n",
        "    num = index % 5\n",
        "  elif index < 10:\n",
        "    is_play = True\n",
        "    num = (index - 5) % 5\n",
        "  val = index - 10\n",
        "  player = val // 10\n",
        "  val = val % 10\n",
        "  is_color = val // 5\n",
        "  val = val % 5\n",
        "  for i in range(len(possible_moves)):\n",
        "\n",
        "    move = str(possible_moves[i])\n",
        "    # print(move)\n",
        "    if move[1] == 'D':\n",
        "      if not is_discard:\n",
        "        continue\n",
        "      if num == int(move[9]):\n",
        "        return possible_moves[i]\n",
        "    elif move[1] == 'P':\n",
        "      if not is_play:\n",
        "        continue\n",
        "      if num == int(move[6]):\n",
        "        return possible_moves[i]\n",
        "    elif move[1] == 'R':\n",
        "      \n",
        "      cur_player = int(move[16]) - 1\n",
        "      cur_is_color = 1 if \"color\" in move else 0\n",
        "      cur_val = int(move[23]) - 1 if cur_is_color == 0 else color_to_int(move[24])\n",
        "      # print(10 + cur_player * 10 + 5 * cur_is_color + cur_val)\n",
        "      if cur_player == player and cur_is_color == is_color and cur_val == val:\n",
        "        return possible_moves[i]\n",
        "  # print(possible_moves, index)\n",
        "  return None\n",
        "\n",
        "# def encode_state(state):\n",
        "#   obs = state.observation(state.cur_player())\n",
        "#   life_input = np.zeros((4, ))\n",
        "#   life_input[obs.life_tokens()] = 1\n",
        "#   info_input = np.zeros((9, ))\n",
        "#   info_input[obs.information_tokens()] = 1\n",
        "#   hand_information = None#encode_observation(obs)\n",
        "#   return [life_input, info_input, hand_information]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTCqiZRtSEYk",
        "outputId": "cc892ee7-37b4-4224-c2cf-6d5aedc62808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Game done. Terminal state:\n",
            "\n",
            "Life tokens: 0\n",
            "Info tokens: 0\n",
            "Fireworks: R1 Y0 G0 W1 B0 \n",
            "Hands:\n",
            "G4 || G4|G4\n",
            "B3 || X3|RYWB3\n",
            "G1 || G1|G1\n",
            "Y3 || XX|RYWB1235\n",
            "Y4 || X4|RYGWB4\n",
            "-----\n",
            "R2 || RX|R2345\n",
            "B1 || X1|WB1\n",
            "G1 || X1|RYGWB1\n",
            "B5 || XX|RYGWB12345\n",
            "-----\n",
            "Y2 || XX|RYG2345\n",
            "R5 || XX|RYG2345\n",
            "B2 || BX|B2345\n",
            "Y1 || X1|RYGWB1\n",
            "W5 || XX|RYGWB2345\n",
            "Deck size: 26\n",
            "Discards: W4 B3 R4 R3 Y4 W4 R1 G4\n",
            "\n",
            "score: 0\n"
          ]
        }
      ],
      "source": [
        "# #Environment\n",
        "# from hanabi_learning_environment import pyhanabi\n",
        "\n",
        "# # Copyright 2018 Google LLC\n",
        "# #\n",
        "# # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# # you may not use this file except in compliance with the License.\n",
        "# # You may obtain a copy of the License at\n",
        "# #\n",
        "# #    https://www.apache.org/licenses/LICENSE-2.0\n",
        "# #\n",
        "# # Unless required by applicable law or agreed to in writing, software\n",
        "# # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# # See the License for the specific language governing permissions and\n",
        "# # limitations under the License.\n",
        "\n",
        "# \"\"\"Example code demonstrating the Python Hanabi interface.\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from hanabi_learning_environment import pyhanabi\n",
        "\n",
        "\n",
        "def run_game(game_parameters):\n",
        "  \"\"\"Play a game, selecting random actions.\"\"\"\n",
        "\n",
        "  def print_state(state):\n",
        "    \"\"\"Print some basic information about the state.\"\"\"\n",
        "    print(\"\")\n",
        "    print(\"Current player: {}\".format(state.cur_player()))\n",
        "    print(state)\n",
        "\n",
        "    # Example of more queries to provide more about this state. For\n",
        "    # example, bots could use these methods to to get information\n",
        "    # about the state in order to act accordingly.\n",
        "    print(\"### Information about the state retrieved separately ###\")\n",
        "    print(\"### Information tokens: {}\".format(state.information_tokens()))\n",
        "    print(\"### Life tokens: {}\".format(state.life_tokens()))\n",
        "    print(\"### Fireworks: {}\".format(state.fireworks()))\n",
        "    print(\"### Deck size: {}\".format(state.deck_size()))\n",
        "    print(\"### Discard pile: {}\".format(str(state.discard_pile())))\n",
        "    print(\"### Player hands: {}\".format(str(state.player_hands())))\n",
        "    print(\"\")\n",
        "\n",
        "  def print_observation(observation):\n",
        "    \"\"\"Print some basic information about an agent observation.\"\"\"\n",
        "    print(\"--- Observation ---\")\n",
        "    print(observation)\n",
        "\n",
        "    print(\"### Information about the observation retrieved separately ###\")\n",
        "    print(\"### Current player, relative to self: {}\".format(\n",
        "        observation.cur_player_offset()))\n",
        "    print(\"### Observed hands: {}\".format(observation.observed_hands()))\n",
        "    print(\"### Card knowledge: {}\".format(observation.card_knowledge()))\n",
        "    print(\"### Discard pile: {}\".format(observation.discard_pile()))\n",
        "    print(\"### Fireworks: {}\".format(observation.fireworks()))\n",
        "    print(\"### Deck size: {}\".format(observation.deck_size()))\n",
        "    move_string = \"### Last moves:\"\n",
        "    for move_tuple in observation.last_moves():\n",
        "      move_string += \" {}\".format(move_tuple)\n",
        "    print(move_string)\n",
        "    print(\"### Information tokens: {}\".format(observation.information_tokens()))\n",
        "    print(\"### Life tokens: {}\".format(observation.life_tokens()))\n",
        "    print(\"### Legal moves: {}\".format(observation.legal_moves()))\n",
        "    print(\"--- EndObservation ---\")\n",
        "\n",
        "  def print_encoded_observations(encoder, state, num_players):\n",
        "    print(\"--- EncodedObservations ---\")\n",
        "    print(\"Observation encoding shape: {}\".format(encoder.shape()))\n",
        "    print(\"Current actual player: {}\".format(state.cur_player()))\n",
        "    for i in range(num_players):\n",
        "      print(\"Encoded observation for player {}: {}\".format(\n",
        "          i, encoder.encode(state.observation(i))))\n",
        "    print(\"--- EndEncodedObservations ---\")\n",
        "\n",
        "  game = pyhanabi.HanabiGame(game_parameters)\n",
        "  # print(game.parameter_string(), end=\"\")\n",
        "  obs_encoder = pyhanabi.ObservationEncoder(game, enc_type=pyhanabi.ObservationEncoderType.CANONICAL)\n",
        "\n",
        "  state = game.new_initial_state()\n",
        "  enc_move = None\n",
        "  while not state.is_terminal():\n",
        "    if state.cur_player() == pyhanabi.CHANCE_PLAYER_ID:\n",
        "      state.deal_random_card()\n",
        "      continue\n",
        "\n",
        "    # print_state(state)\n",
        "\n",
        "    observation = state.observation(state.cur_player())\n",
        "    enc_move = encode_observation(observation)\n",
        "\n",
        "    # print_observation(observation)\n",
        "    # print_encoded_observations(obs_encoder, state, game.num_players())\n",
        "\n",
        "    legal_moves = state.legal_moves()\n",
        "    \n",
        "    moves = encode_possible_moves(legal_moves)\n",
        "    \n",
        "    \n",
        "    t = []\n",
        "    for i in range(30):\n",
        "      if moves[i] == 0:\n",
        "        continue\n",
        "      move = possible_move_from_index(legal_moves, i)\n",
        "      if move == None:\n",
        "        # print(i)\n",
        "        # print(legal_moves)\n",
        "        # print(moves)\n",
        "        return\n",
        "      t.append(possible_move_from_index(legal_moves, i))\n",
        "      # print(possible_move_from_index(legal_moves, i))\n",
        "    # print(len(t))\n",
        "    # print(len(legal_moves))\n",
        "    # print(\"\")\n",
        "    # print(\"Number of legal moves: {}\".format(len(legal_moves)))\n",
        "\n",
        "    move = np.random.choice(legal_moves)\n",
        "    # print(observation.last_moves())\n",
        "    # print(\"Chose random legal move: {}\".format(move))\n",
        "    # print(state.score())\n",
        "    # print(state.cur_player())\n",
        "    state.apply_move(move)\n",
        "    # print(state.is_terminal())\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Game done. Terminal state:\")\n",
        "  print(\"\")\n",
        "  print(state)\n",
        "  print(\"\")\n",
        "  print(\"score: {}\".format(state.score()))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Check that the cdef and library were loaded from the standard paths.\n",
        "  assert pyhanabi.cdef_loaded(), \"cdef failed to load\"\n",
        "  assert pyhanabi.lib_loaded(), \"lib failed to load\"\n",
        "  run_game({\"players\": 3, \"random_start_player\": True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oeGba8DdTsB",
        "outputId": "1097c1fc-52a9-46fc-cdb0-b2d346e393d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "V3hCLEgHV6ih"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import keras\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "import numpy.ma as ma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1, 2 ,3 ,4]\n",
        "print(a[0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKbJZ-LJkQrM",
        "outputId": "68129ce9-5203-464c-e1c5-7102491dbb87"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN_Agent:\n",
        "    #\n",
        "    # Initializes attributes and constructs CNN model and target_model\n",
        "    #\n",
        "    def __init__(self, action_size, self_talk_size, uncertain=False):\n",
        "        self.action_size = action_size\n",
        "        self.self_talk_size = self_talk_size\n",
        "        self.memory = deque(maxlen=5000)\n",
        "        self.uncertain = uncertain\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.gamma = 0.9            # Discount rate\n",
        "        self.epsilon = 1.0          # Exploration rate\n",
        "        self.epsilon_min = 0.1      # Minimal exploration rate (epsilon-greedy)\n",
        "        self.epsilon_decay = 0.9  # Decay rate for epsilon\n",
        "        # self.update_rate = 1     # Number of steps until updating the target network\n",
        "        \n",
        "        # Construct DQN models\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    #\n",
        "    # Constructs CNN\n",
        "    #\n",
        "    def _build_model(self):\n",
        "      \n",
        "      life_input = keras.Input(shape=(4,), name=\"life\")\n",
        "      info_input = keras.Input(shape=(9,), name=\"info\")\n",
        "      hand_input = keras.Input(shape=(3, 5, 5, 5, 3), name=\"hand\")\n",
        "      talk_input = keras.Input(shape=(self.self_talk_size,), name=\"talk\")\n",
        "\n",
        "      hand_outputs = []\n",
        "      for i in range(3):\n",
        "        card_outputs = []\n",
        "        for j in range(5):\n",
        "          out = Lambda(lambda x: x[:, i, j, :, :, :])(hand_input)\n",
        "          out = Conv2D(filters=1, kernel_size=3, activation='relu')(out)\n",
        "          out = Flatten()(out)\n",
        "          out = Dense(32, activation='relu')(out)\n",
        "          card_outputs.append(out)\n",
        "        out = Concatenate()(card_outputs)\n",
        "        out = Dense(32, activation='relu')(out)\n",
        "        hand_outputs.append(out)\n",
        "      hand_outputs.append(life_input)\n",
        "      hand_outputs.append(info_input)\n",
        "      hand_outputs.append(talk_input)\n",
        "      out = Concatenate()(hand_outputs)\n",
        "      out = Dense(64, activation='relu')(out)\n",
        "      out = Dense(64, activation='relu')(out)\n",
        "      actions_out = Dense(self.action_size + self.self_talk_size, activation='sigmoid')(out)\n",
        "      # talk_out = Dense(self.self_talk_size, activation = 'sigmoid')(out)\n",
        "      model = keras.Model(\n",
        "        inputs=[life_input, info_input, hand_input, talk_input],\n",
        "        outputs=[actions_out],\n",
        "      )\n",
        "      model.compile(loss='mse', optimizer=Adam())\n",
        "      return model\n",
        "\n",
        "    #\n",
        "    # Stores experience in replay memory\n",
        "    #\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def get_epsilon(self):\n",
        "      return self.epsilon\n",
        "    #\n",
        "    # Chooses action based on epsilon-greedy policy\n",
        "    #\n",
        "    def act(self, state, enc_legal_moves):\n",
        "        # Random exploration\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "          idxs = enc_legal_moves.nonzero()\n",
        "          return [random.choice(idxs[0]), np.zeros(self.self_talk_size)]\n",
        "        \n",
        "        act_talk_values = self.model.predict(state, verbose = 0)\n",
        "        act_values = act_talk_values[0][0:self.action_size]\n",
        "        \n",
        "        # act_values += np.argmin(act_values)\n",
        "        # print(act_values)\n",
        "        act_values = act_values * enc_legal_moves\n",
        "        uncertainty = 1\n",
        "        if self.uncertain:\n",
        "          maxes = act_values[np.argpartition(act_values, -5)[-5:]]\n",
        "          mean = sum(maxes) / len(maxes)\n",
        "          uncertainty = (1 + (sum([((x - mean) ** 2) for x in maxes]) / len(maxes)) ** 0.5)\n",
        "        # print(enc_legal_moves)\n",
        "        return [np.argmax(act_values), act_talk_values[0][self.action_size:] * uncertainty]  # Returns action using policy\n",
        "\n",
        "    #\n",
        "    # Trains the model using randomly selected experiences in the replay memory\n",
        "    #\n",
        "    def replay(self, batch_size):\n",
        "        \n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        \n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            # print(self.target_model.predict(next_state, verbose=0))\n",
        "            if not done:\n",
        "                target = reward + self.gamma * np.amax(self.target_model.predict(next_state, verbose=0))\n",
        "            else:\n",
        "                target = reward\n",
        "\n",
        "            # print('test')\n",
        "            # Construct the target vector as follows:\n",
        "            # 1. Use the current model to output the Q-value predictions\n",
        "            target_f = self.model.predict(state, verbose=0)\n",
        "            \n",
        "            # 2. Rewrite the chosen action value with the computed target\n",
        "            target_f[0][action[0]] = target\n",
        "            \n",
        "            # 3. Use vectors in the objective computation\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "            \n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    #\n",
        "    # Sets the target model parameters to the current model parameters\n",
        "    #\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "            \n",
        "    #\n",
        "    # Loads a saved model\n",
        "    #\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    #\n",
        "    # Saves parameters of a trained model\n",
        "    #\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "metadata": {
        "id": "kzITlR8QWxX4"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "pBS1HKTgY0oA"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "def train_agent(game_parameters, episodes):\n",
        "  action_size = 30 \n",
        "  self_talk_size = 4\n",
        "  agent = DQN_Agent(action_size, self_talk_size)\n",
        "\n",
        "  batch_size = 8\n",
        "  total_time = 0   # Counter for total number of steps taken\n",
        "  all_rewards = 0  # Used to compute avg reward over time\n",
        "  game = pyhanabi.HanabiGame(game_parameters)\n",
        "  \n",
        "  possible_moves = None\n",
        "  game_scores = [] #has length episodes\n",
        "  for e in range(episodes):\n",
        "    print(e)\n",
        "    print(agent.get_epsilon())\n",
        "    # print(game.parameter_string(), end=\"\")\n",
        "    # obs_encoder = pyhanabi.ObservationEncoder(game, enc_type=pyhanabi.ObservationEncoderType.CANONICAL)\n",
        "\n",
        "    state = game.new_initial_state()\n",
        "    enc_obs = None\n",
        "    game_score = 0\n",
        "    done = False\n",
        "\n",
        "    # enc_obs = [None, None, None]\n",
        "    actions = [0, 0, 0]\n",
        "\n",
        "    while not state.is_terminal():\n",
        "      player = state.cur_player()\n",
        "      if player == pyhanabi.CHANCE_PLAYER_ID:\n",
        "        state.deal_random_card()\n",
        "        continue\n",
        "      \n",
        "      total_time += 1\n",
        "      if enc_obs == None:\n",
        "        enc_obs = encode_observation(state.observation(state.cur_player()))\n",
        "        enc_obs[\"talk\"] = np.zeros((1, self_talk_size))\n",
        "      # Every update_rate timesteps we update the target network parameters\n",
        "      # if total_time % agent.update_rate == 0:\n",
        "      # print('test5')\n",
        "      agent.update_target_model()\n",
        "      legal_moves = state.legal_moves()\n",
        "      # print('test4')\n",
        "      enc_legal_moves = encode_possible_moves(legal_moves)\n",
        "      # print('test3')\n",
        "      # Transition Dynamics\n",
        "      action = agent.act(enc_obs, enc_legal_moves)\n",
        "      # actions[player] = action\n",
        "      move = possible_move_from_index(legal_moves, action[0])\n",
        "      \n",
        "      state.apply_move(move)\n",
        "      done = state.is_terminal()\n",
        "      \n",
        "      while state.cur_player() == pyhanabi.CHANCE_PLAYER_ID:\n",
        "        state.deal_random_card()\n",
        "      \n",
        "      score = state.score()\n",
        "      game_score = score if score > game_score else game_score\n",
        "      reward = game_score if done else 0\n",
        "\n",
        "      new_enc_obs = encode_observation(state.observation(state.cur_player()))\n",
        "      # print(action[1])\n",
        "      new_enc_obs[\"talk\"] = action[1].reshape((1, self_talk_size))\n",
        "      agent.remember(enc_obs, action, reward, new_enc_obs, done)\n",
        "      enc_obs = new_enc_obs\n",
        "\n",
        "      if done:\n",
        "          game_scores.append(game_score)\n",
        "          all_rewards += game_score\n",
        "          \n",
        "          moving_avg_score = sum(game_scores[-10:])/10 if len(game_scores) >= 10 else sum(game_scores) / len(game_scores)\n",
        "          print(\"episode: {}/{}, game score: {}, moving avg reward: {}, total time: {}\"\n",
        "                .format(e+1, episodes, game_score, moving_avg_score, total_time))\n",
        "          \n",
        "          break\n",
        "      # Store sequence in replay memory\n",
        "          \n",
        "      if len(agent.memory) > batch_size:\n",
        "        agent.replay(batch_size)\n",
        "  return game_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def train_agent_uncertain_talk(game_parameters, episodes):\n",
        "  action_size = 30 \n",
        "  self_talk_size = 4\n",
        "  agent = DQN_Agent(action_size, self_talk_size, uncertain=True)\n",
        "\n",
        "  batch_size = 8\n",
        "  total_time = 0   # Counter for total number of steps taken\n",
        "  all_rewards = 0  # Used to compute avg reward over time\n",
        "  game = pyhanabi.HanabiGame(game_parameters)\n",
        "  \n",
        "  possible_moves = None\n",
        "  game_scores = [] #has length episodes\n",
        "  for e in range(episodes):\n",
        "    print(e)\n",
        "    print(agent.get_epsilon())\n",
        "    # print(game.parameter_string(), end=\"\")\n",
        "    # obs_encoder = pyhanabi.ObservationEncoder(game, enc_type=pyhanabi.ObservationEncoderType.CANONICAL)\n",
        "\n",
        "    state = game.new_initial_state()\n",
        "    enc_obs = None\n",
        "    game_score = 0\n",
        "    done = False\n",
        "\n",
        "    # enc_obs = [None, None, None]\n",
        "    actions = [0, 0, 0]\n",
        "\n",
        "    while not state.is_terminal():\n",
        "      player = state.cur_player()\n",
        "      if player == pyhanabi.CHANCE_PLAYER_ID:\n",
        "        state.deal_random_card()\n",
        "        continue\n",
        "      \n",
        "      total_time += 1\n",
        "      if enc_obs == None:\n",
        "        enc_obs = encode_observation(state.observation(state.cur_player()))\n",
        "        enc_obs[\"talk\"] = np.zeros((1, self_talk_size))\n",
        "      # Every update_rate timesteps we update the target network parameters\n",
        "      # if total_time % agent.update_rate == 0:\n",
        "      # print('test5')\n",
        "      agent.update_target_model()\n",
        "      legal_moves = state.legal_moves()\n",
        "      # print('test4')\n",
        "      enc_legal_moves = encode_possible_moves(legal_moves)\n",
        "      # print('test3')\n",
        "      # Transition Dynamics\n",
        "      action = agent.act(enc_obs, enc_legal_moves)\n",
        "      # actions[player] = action\n",
        "      move = possible_move_from_index(legal_moves, action[0])\n",
        "      \n",
        "      state.apply_move(move)\n",
        "      done = state.is_terminal()\n",
        "      \n",
        "      while state.cur_player() == pyhanabi.CHANCE_PLAYER_ID:\n",
        "        state.deal_random_card()\n",
        "      \n",
        "      score = state.score()\n",
        "      game_score = score if score > game_score else game_score\n",
        "      reward = game_score if done else 0\n",
        "\n",
        "      new_enc_obs = encode_observation(state.observation(state.cur_player()))\n",
        "      # print(action[1])\n",
        "      new_enc_obs[\"talk\"] = action[1].reshape((1, self_talk_size))\n",
        "      agent.remember(enc_obs, action, reward, new_enc_obs, done)\n",
        "      enc_obs = new_enc_obs\n",
        "\n",
        "      if done:\n",
        "          game_scores.append(game_score)\n",
        "          all_rewards += game_score\n",
        "          \n",
        "          moving_avg_score = sum(game_scores[-10:])/10 if len(game_scores) >= 10 else sum(game_scores) / len(game_scores)\n",
        "          print(\"episode: {}/{}, game score: {}, moving avg reward: {}, total time: {}\"\n",
        "                .format(e+1, episodes, game_score, moving_avg_score, total_time))\n",
        "          \n",
        "          break\n",
        "      # Store sequence in replay memory\n",
        "          \n",
        "      if len(agent.memory) > batch_size:\n",
        "        agent.replay(batch_size)\n",
        "  return game_scores"
      ],
      "metadata": {
        "id": "lsidTmFJTmBq"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "vOCmsL5xIb5y",
        "outputId": "b4fa1c2f-fa40-4ce1-9ac6-d547d52104ab"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-526498a73401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Check that the cdef and library were loaded from the standard paths.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mpyhanabi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdef_loaded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cdef failed to load\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mpyhanabi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib_loaded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib failed to load\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mgame_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_agent_uncertain_talk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"players\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_start_player\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pyhanabi' is not defined"
          ]
        }
      ],
      "source": [
        "game_scores = []\n",
        "episodes = 200\n",
        "if __name__ == \"__main__\":\n",
        "  # Check that the cdef and library were loaded from the standard paths.\n",
        "  assert pyhanabi.cdef_loaded(), \"cdef failed to load\"\n",
        "  assert pyhanabi.lib_loaded(), \"lib failed to load\"\n",
        "  game_scores = train_agent_uncertain_talk({\"players\": 3, \"random_start_player\": True}, episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "aAkaTxd9rtnH"
      },
      "outputs": [],
      "source": [
        "moving_avg_game_scores = []\n",
        "for i in range(episodes - 10):\n",
        "  moving_avg_game_scores.append(sum(game_scores[i:i + 10]) / 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "4GGNb0IWtg9A",
        "outputId": "f3247025-27a0-444e-c5d1-4666d7d284bc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1528980aa000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoving_avg_game_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epiodes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DQN with Uncertainity Memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "plt.plot(game_scores)\n",
        "plt.plot(moving_avg_game_scores)\n",
        "plt.xlabel(\"Epiodes\")\n",
        "plt.ylabel(\"Average Score\")\n",
        "plt.title(\"DQN with Uncertainity Memory\")\n",
        "plt.savefig(\"UNc\", dpi = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vJ5Lg56mhidz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}